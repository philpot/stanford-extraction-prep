This module converts stanford extractions to sequence files to be modeled by Karma.

philpot 10 November 2015

Stanford generates tab-separated data rows that look like this:

<sourceName>:<crawlId>\t<payload>

(phone, service, ismassageparlor)

e.g.

adultsearch:10024       (856) 676-4184
adultsearch:10025       (347) 460-7854

or

<auxID>\t<sourceName>:<crawlId>\t<payload>

(rates, email)

e.g.

rates_7598613   adultsearch:112 200,30 MINS
rates_7605289   adultsearch:11096       150,1 HOUR

We ignore the auxID.  Also see discussion of -k/--shift below.

There are Stanford extractions named movement and organized.  These
have no payload, merely the <sourceName>:<crawlId> pair.  Presumably
this can be used to create groups/clusters.  We are ignoring these for
now.

In some cases, the payload is really two related values separated by comma.

This spark job joins the <sourceName>:<crawlId> in the indicated input
file with a selected CDR-derived sequence file.  The CDR sequence data
rows are expected to be <url>\t<json>.  The JSON must have fields:

_source.sources_id
_source.incoming_id
_source.id

and should have field

 _source.url

The sources_id and sourceName fields are 1:1 and a mapping table to
this effect generated by Dipsy from dig-alignment/ht version 1 was
adapted for performing the mapping.  The incoming_id is referred to
above as crawlId.  The id is the output database id.  The url field
extracted from the payload should be equally available as the tsv key
field.

The Spark job performs an left outer join from the stanford data to
the CDR data.  This means that any Stanford record which does not
match a CDR record will be assigned NULL as CDR data.  Matching
records are deemed success cases and any NULL records are deemed
failure cases but are retained.

Command line arguments

-h: help
-v: verbose
-c: location of CDR data
-s: location of Stanford data
-g: directory to write successful data (conventional output)
-f: directory to write failure data (join misses)
-k: number of columns to shift left.  Typically 0, set to 1 to drop the <auxId> field from rates, email.
-u: ignored
-p: how many Spark partitions to assign
-n: can be set to the name of the task for tracking/distinguishing calls between running jobs
-l: for debugging: consider only approximately this number of records
-z: for debugging: if 2, spill rdd contents to named file at each rdd computed; if 1, dump only partition information; if 0, no information
-x: for debugging: focus on a single known record only
-y: outputFormat: may be text, sequence, tsv, or newSequence (= compressed sequence file using new Hadoop API: under development)


Counts of stanford extractions (escort directory)
 433922 email_addresses.tsv
 29907390 ismassageparlorad.tsv
 13854857 phone_numbers.tsv
 7598570 rates.tsv
 29907390 service.tsv

Estimated yield when joining with ht20mil

email: 46% or roughly 199600
massage: 53% or roughly 15850000
phone numbers: 55% or roughly 7620000
rate: 52% or roughly 3951000
service (incall/outcall/etc.): 16449000